{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1>Creator of Notebook: Shehryar Mallick</h1>\n<h1>This notebook contains implementations to determine whether the individuals will leave a bank of a US bank</h1>","metadata":{}},{"cell_type":"markdown","source":"<h1>Data Preprocessing Stage</h1>","metadata":{}},{"cell_type":"markdown","source":"<h2>Exploratory Data Analysis to gain insights and Data Wrangling techniques</h2>\n<h3> checking for unique values in a column\n<h3> checking for null values in a column\n<h3> checking for outliers in specific column\n<h3> Encoding categorical columns using one hot encoding\n<h3> Normalizing the data using MinMax Scaler\n<h3> Binning the data columns with high cardinality","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\nprint('import successful')","metadata":{"execution":{"iopub.status.busy":"2022-07-16T06:04:08.740262Z","iopub.execute_input":"2022-07-16T06:04:08.740728Z","iopub.status.idle":"2022-07-16T06:04:08.772650Z","shell.execute_reply.started":"2022-07-16T06:04:08.740636Z","shell.execute_reply":"2022-07-16T06:04:08.771620Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('../input/bank-customer-churn-prediction/Churn_Modelling.csv')\ndf.head(5)\nprint(df.shape)","metadata":{"execution":{"iopub.status.busy":"2022-07-16T06:04:17.248624Z","iopub.execute_input":"2022-07-16T06:04:17.249023Z","iopub.status.idle":"2022-07-16T06:04:17.300864Z","shell.execute_reply.started":"2022-07-16T06:04:17.248992Z","shell.execute_reply":"2022-07-16T06:04:17.300015Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"### print out all of the column names and the contents in the dataset\ncol_names = df.columns\nprint(col_names,'\\n#########################')\nfor i in col_names:\n  print(\"Column : \",i)\n  display(df[i].value_counts())\n  print(\"#######################\")","metadata":{"execution":{"iopub.status.busy":"2022-07-16T06:04:32.481457Z","iopub.execute_input":"2022-07-16T06:04:32.481838Z","iopub.status.idle":"2022-07-16T06:04:32.581732Z","shell.execute_reply.started":"2022-07-16T06:04:32.481807Z","shell.execute_reply":"2022-07-16T06:04:32.580430Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-07-16T06:04:35.453741Z","iopub.execute_input":"2022-07-16T06:04:35.454561Z","iopub.status.idle":"2022-07-16T06:04:35.469611Z","shell.execute_reply.started":"2022-07-16T06:04:35.454517Z","shell.execute_reply":"2022-07-16T06:04:35.468414Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"### from the initial analysis it seemed logical to drop cols:RowNumber, CustomerId, Surname\n### moreover we have to create bin due to high cardinality and also identify any prevailing outliers of the columns: CreditScore, Age,Balance, EstimatedSalary\n### the columns that need to be encoded are: Geography, Gender","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3>Dropping columns that do not affect the data set","metadata":{}},{"cell_type":"code","source":"df = df.drop(columns=['RowNumber', 'CustomerId', 'Surname'])\ndf","metadata":{"execution":{"iopub.status.busy":"2022-07-16T06:06:15.254866Z","iopub.execute_input":"2022-07-16T06:06:15.255290Z","iopub.status.idle":"2022-07-16T06:06:15.289771Z","shell.execute_reply.started":"2022-07-16T06:06:15.255259Z","shell.execute_reply":"2022-07-16T06:06:15.288662Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"<h3>Outlier Detection using\n    <h4>Box plot technique\n    <h4> Z score technique\n<h3>Outlier removal by setting threshold","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nfig, axes = plt.subplots(2, 2,figsize=(16,16))\n\nsns.boxplot(df['CreditScore'], ax=axes[0,0]).set(title='CreditScore')\nsns.boxplot(df['Age'], ax=axes[0,1]).set(title='Age')\nsns.boxplot(df['EstimatedSalary'], ax=axes[1,0]).set(title='EstimatedSalary')\nsns.boxplot(df['Balance'], ax=axes[1,1]).set(title='Balance')","metadata":{"execution":{"iopub.status.busy":"2022-07-16T06:06:55.632087Z","iopub.execute_input":"2022-07-16T06:06:55.632494Z","iopub.status.idle":"2022-07-16T06:06:56.768766Z","shell.execute_reply.started":"2022-07-16T06:06:55.632464Z","shell.execute_reply":"2022-07-16T06:06:56.767651Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"### z score to detect outliers\n\nfrom scipy import stats\nimport numpy as np\nz = np.abs(stats.zscore(df[['CreditScore', 'Age','Balance', 'EstimatedSalary']]))\n# print(z)    ###  to check the z score of the mentioned columns\n\nthreshold = 3\nprint(np.where(z > 3))","metadata":{"execution":{"iopub.status.busy":"2022-07-16T06:07:20.608668Z","iopub.execute_input":"2022-07-16T06:07:20.609053Z","iopub.status.idle":"2022-07-16T06:07:20.624745Z","shell.execute_reply.started":"2022-07-16T06:07:20.609023Z","shell.execute_reply":"2022-07-16T06:07:20.623508Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"print(df.shape)\ndf= df[(z < 3).all(axis=1)]    ### we are now eliminating the rows that possess the outliers\nprint(df.shape)","metadata":{"execution":{"iopub.status.busy":"2022-07-16T06:07:25.742704Z","iopub.execute_input":"2022-07-16T06:07:25.743159Z","iopub.status.idle":"2022-07-16T06:07:25.752554Z","shell.execute_reply.started":"2022-07-16T06:07:25.743097Z","shell.execute_reply":"2022-07-16T06:07:25.751594Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"<h2>Making copies of Data to explore variety of techniques","metadata":{}},{"cell_type":"code","source":"df_o = df.copy()   ### we will use this data for binning technique and without normalization\ndf_normal = df.copy() ### we will use this dataset with normalization using minmax scaler","metadata":{"execution":{"iopub.status.busy":"2022-07-16T08:25:54.569680Z","iopub.execute_input":"2022-07-16T08:25:54.570127Z","iopub.status.idle":"2022-07-16T08:25:54.578223Z","shell.execute_reply.started":"2022-07-16T08:25:54.570089Z","shell.execute_reply":"2022-07-16T08:25:54.576670Z"},"trusted":true},"execution_count":133,"outputs":[]},{"cell_type":"code","source":"import category_encoders as ce   #for importing the encoder\nfrom sklearn.preprocessing import MinMaxScaler\n\ndf_normal['Gender'] = df_normal['Gender'].replace(['Male','Female'],[1,0])\n\n#Create object for one-hot encoding\nOH_encoder=ce.OneHotEncoder(cols='Geography',handle_unknown='return_nan',return_df=True,use_cat_names=True) #for geography col\n\n# encode dataset\ndf_normal = OH_encoder.fit_transform(df_normal)\n# df_normal\n\nscaler = MinMaxScaler()\ndf_normal[['CreditScore','Age','Balance','EstimatedSalary']] = scaler.fit_transform(df_normal[['CreditScore','Age','Balance','EstimatedSalary']])\ndf_normal.head(3)","metadata":{"execution":{"iopub.status.busy":"2022-07-16T08:26:05.967388Z","iopub.execute_input":"2022-07-16T08:26:05.968393Z","iopub.status.idle":"2022-07-16T08:26:06.044223Z","shell.execute_reply.started":"2022-07-16T08:26:05.968353Z","shell.execute_reply":"2022-07-16T08:26:06.043033Z"},"trusted":true},"execution_count":134,"outputs":[]},{"cell_type":"code","source":"bin_cols = ['CreditScore', 'Age','Balance', 'EstimatedSalary']\nfor i in bin_cols:\n    print(i,'\\n',df[i].min(),'\\n',df[i].max(),'\\n__________________')","metadata":{"execution":{"iopub.status.busy":"2022-07-16T06:07:54.302703Z","iopub.execute_input":"2022-07-16T06:07:54.303113Z","iopub.status.idle":"2022-07-16T06:07:54.312879Z","shell.execute_reply.started":"2022-07-16T06:07:54.303082Z","shell.execute_reply":"2022-07-16T06:07:54.311886Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"<h2>Binnig for three columns was conducted using Strudes Rule","metadata":{}},{"cell_type":"code","source":"### bins for credit score, for the selection of num of bins we used Sturgeâ€™s rule K = 1+3.322log(N)\n### where N = 9859 observations. hence K=14\n\nfrom sklearn.preprocessing import KBinsDiscretizer   ###importing binsmaker from sklear\n\nest = KBinsDiscretizer(n_bins=14, encode='ordinal',strategy='uniform')\ndf_o['CreditScore'] = est.fit_transform(df_o[['CreditScore']])\n\nfig, axes = plt.subplots(1, 2,figsize=(15,7))\nsns.distplot(df[['CreditScore']], ax=axes[0]).set(title='CreditScore before binning')\nsns.distplot(df_o['CreditScore'],ax=axes[1]).set(title='CreditScore after binning')\n","metadata":{"execution":{"iopub.status.busy":"2022-07-16T06:07:57.831307Z","iopub.execute_input":"2022-07-16T06:07:57.831712Z","iopub.status.idle":"2022-07-16T06:07:58.518202Z","shell.execute_reply.started":"2022-07-16T06:07:57.831682Z","shell.execute_reply":"2022-07-16T06:07:58.516912Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"### bins for Age\n\nest = KBinsDiscretizer(n_bins=5, encode='ordinal',strategy='uniform')\ndf_o['Age'] = est.fit_transform(df_o[['Age']])\n\nfig, axes = plt.subplots(1, 2,figsize=(15,7))\nsns.distplot(df[['Age']], ax=axes[0]).set(title='Age before binning')\nsns.distplot(df_o['Age'],ax=axes[1]).set(title='Age after binning')","metadata":{"execution":{"iopub.status.busy":"2022-07-16T06:08:03.156014Z","iopub.execute_input":"2022-07-16T06:08:03.156462Z","iopub.status.idle":"2022-07-16T06:08:03.708213Z","shell.execute_reply.started":"2022-07-16T06:08:03.156429Z","shell.execute_reply":"2022-07-16T06:08:03.707230Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"### bins for Balance\n\nest = KBinsDiscretizer(n_bins=14, encode='ordinal',strategy='uniform')\ndf_o['Balance'] = est.fit_transform(df_o[['Balance']])\n\nfig, axes = plt.subplots(1, 2,figsize=(15,7))\nsns.distplot(df[['Balance']], ax=axes[0]).set(title='Balance before binning')\nsns.distplot(df_o['Balance'],ax=axes[1]).set(title='Balance after binning')","metadata":{"execution":{"iopub.status.busy":"2022-07-16T06:08:08.490756Z","iopub.execute_input":"2022-07-16T06:08:08.491151Z","iopub.status.idle":"2022-07-16T06:08:09.038293Z","shell.execute_reply.started":"2022-07-16T06:08:08.491100Z","shell.execute_reply":"2022-07-16T06:08:09.037440Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"### bins for EstimatedSalary\n\nest = KBinsDiscretizer(n_bins=14, encode='ordinal',strategy='uniform')\ndf_o['EstimatedSalary'] = est.fit_transform(df_o[['EstimatedSalary']])\n\nfig, axes = plt.subplots(1, 2,figsize=(15,7))\nsns.distplot(df[['EstimatedSalary']], ax=axes[0]).set(title='EstimatedSalary before binning')\nsns.distplot(df_o['EstimatedSalary'],ax=axes[1]).set(title='EstimatedSalary after binning')\n","metadata":{"execution":{"iopub.status.busy":"2022-07-16T06:08:14.778770Z","iopub.execute_input":"2022-07-16T06:08:14.779200Z","iopub.status.idle":"2022-07-16T06:08:15.269486Z","shell.execute_reply.started":"2022-07-16T06:08:14.779165Z","shell.execute_reply":"2022-07-16T06:08:15.268215Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"import category_encoders as ce   #for importing the encoder\n\ndf_o['Gender'] = df_o['Gender'].replace(['Male','Female'],[1,0])\n\n#Create object for one-hot encoding\nOH_encoder=ce.OneHotEncoder(cols='Geography',handle_unknown='return_nan',return_df=True,use_cat_names=True) #for geography col\n\n# encode dataset\ndata_encoded = OH_encoder.fit_transform(df_o)\ndata_encoded","metadata":{"execution":{"iopub.status.busy":"2022-07-16T06:09:39.422436Z","iopub.execute_input":"2022-07-16T06:09:39.422842Z","iopub.status.idle":"2022-07-16T06:09:39.710349Z","shell.execute_reply.started":"2022-07-16T06:09:39.422811Z","shell.execute_reply":"2022-07-16T06:09:39.709192Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"<h1>DATA VISUALIZATION</h1>","metadata":{}},{"cell_type":"markdown","source":"<h2>Visualizing the correlation between different feature variables and target variable","metadata":{}},{"cell_type":"code","source":"### plot the heatmap of correlation between different columns\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.figure(figsize=(14, 8))\ncorr = data_encoded.corr()\nsns.heatmap(corr, annot=True, center=0, linewidths=.5)\nplt.title('Bank Churn Dataset Correlation', fontsize=16)","metadata":{"execution":{"iopub.status.busy":"2022-07-16T08:59:50.025224Z","iopub.execute_input":"2022-07-16T08:59:50.025987Z","iopub.status.idle":"2022-07-16T08:59:51.053270Z","shell.execute_reply.started":"2022-07-16T08:59:50.025922Z","shell.execute_reply":"2022-07-16T08:59:51.052396Z"},"trusted":true},"execution_count":144,"outputs":[]},{"cell_type":"markdown","source":"<h2>Visualization to determine the relation of the \"CHURN\" against individual feature","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(4, 3,figsize=(25,15))\n\ndf_cols_name = list(data_encoded.columns)\ndf_cols_name.pop()\n\ninc = 0\nfor i in range(4):\n    for j in range(3):\n        sns.countplot(x='Exited',data=data_encoded,hue=df_cols_name[inc],palette=\"pastel\",ax=axes[i,j]).legend(fontsize=7,loc='upper right').set_title(df_cols_name[inc],prop={'size':9})\n        inc+=1","metadata":{"execution":{"iopub.status.busy":"2022-07-16T06:41:54.552259Z","iopub.execute_input":"2022-07-16T06:41:54.552678Z","iopub.status.idle":"2022-07-16T06:41:56.492934Z","shell.execute_reply.started":"2022-07-16T06:41:54.552644Z","shell.execute_reply":"2022-07-16T06:41:56.491726Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"markdown","source":"<h1>Machine Learning Phase","metadata":{}},{"cell_type":"markdown","source":"<h2>Splitting the data into Feature set and Target Variable","metadata":{}},{"cell_type":"code","source":"### features and target comprising of bins data\ny = data_encoded['Exited']\nX = data_encoded.drop(columns=['Exited'])\nprint(X.shape,y.shape)\n\n### features and target comprising of normalized data\ny_norm = df_normal['Exited']\nX_norm = df_normal.drop(columns=['Exited'])\nprint(X_norm.shape,y_norm.shape)","metadata":{"execution":{"iopub.status.busy":"2022-07-16T08:26:57.933871Z","iopub.execute_input":"2022-07-16T08:26:57.934365Z","iopub.status.idle":"2022-07-16T08:26:57.944717Z","shell.execute_reply.started":"2022-07-16T08:26:57.934327Z","shell.execute_reply":"2022-07-16T08:26:57.943876Z"},"trusted":true},"execution_count":135,"outputs":[]},{"cell_type":"markdown","source":"<h2> I have used Cross validation technique with cv=5 folds\n<h2> The evaluation metrics used are:\n    <h3>Accuracy'\n    <h3>precision\n    <h3>precision_macro\n    <h3>recall\n    <h3>recall_macro\n    <h3>roc_auc\n    <h3>f1\n        \n<h2> The Machine Learning Algorithms used are:\n    <h3>K-Nearest Neighbours\n    <h3>Decision Tree\n    <h3>Random Forest\n    <h3>Logistic Regression\n    <h3>Gaussian NaÃ¯ve Bayes\n    <h3>XGBOOST\n        \n\n    ","metadata":{}},{"cell_type":"code","source":"### all the avialable scoring options\n\n# import sklearn \n# sklearn.metrics.SCORERS.keys()","metadata":{"execution":{"iopub.status.busy":"2022-07-16T07:43:25.702898Z","iopub.execute_input":"2022-07-16T07:43:25.704005Z","iopub.status.idle":"2022-07-16T07:43:25.708017Z","shell.execute_reply.started":"2022-07-16T07:43:25.703956Z","shell.execute_reply":"2022-07-16T07:43:25.707209Z"},"trusted":true},"execution_count":88,"outputs":[]},{"cell_type":"code","source":"### K-Nearest Neighbours\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import cross_validate\n# from sklearn.metrics import recall_score\n# from sklearn.metrics import classification_report\n# from sklearn.metrics import confusion_matrix\n# from sklearn.metrics import roc_auc_score\n\n# for n in range (10,16):\n#     neigh = KNeighborsClassifier(n_neighbors=n)\n#     scores = cross_val_score(neigh, X, y,cv=5)\n#     print(scores)\n#     print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n\nneigh = KNeighborsClassifier(n_neighbors=12)\n# accuracy = cross_val_score(neigh, X, y,cv=5)\n# print(accuracy)\n# print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (accuracy.mean(), accuracy.std()))\n\nscoring = ['accuracy','precision','precision_macro','recall','recall_macro','roc_auc','f1']\nscores = cross_validate(neigh, X, y.ravel(), scoring=scoring)\nscoreKeys = sorted(scores.keys())\nprint('DATA SET WITH BINNING AND NO NORMALIZATION')\nfor key in scoreKeys:\n    print(key,':',scores[key].mean())\n\nprint('---------------------------------------------------------------------------')\nscores = cross_validate(neigh, X_norm, y_norm.ravel(), scoring=scoring)\nscoreKeys = sorted(scores.keys())\nprint('DATA SET WITH NO BINNING AND NORMALIZATION')\nfor key in scoreKeys:\n    print(key,':',scores[key].mean())","metadata":{"execution":{"iopub.status.busy":"2022-07-16T08:28:49.815114Z","iopub.execute_input":"2022-07-16T08:28:49.816243Z","iopub.status.idle":"2022-07-16T08:28:52.607807Z","shell.execute_reply.started":"2022-07-16T08:28:49.816198Z","shell.execute_reply":"2022-07-16T08:28:52.606688Z"},"trusted":true},"execution_count":136,"outputs":[]},{"cell_type":"code","source":"### Decision Tree\n\nfrom sklearn.tree import DecisionTreeClassifier\nDT = DecisionTreeClassifier(random_state=0)\n\nscoring = ['accuracy','precision','precision_macro','recall','recall_macro','roc_auc','f1']\nscores = cross_validate(DT, X, y.ravel(), scoring=scoring)\nscoreKeys = sorted(scores.keys())\nprint('DATA SET WITH BINNING AND NO NORMALIZATION')\nfor key in scoreKeys:\n    print(key,':',scores[key].mean())\n    \nprint('---------------------------------------------------------------------------')\nscores = cross_validate(DT, X_norm, y_norm.ravel(), scoring=scoring)\nscoreKeys = sorted(scores.keys())\nprint('DATA SET WITH NO BINNING AND NORMALIZATION')\nfor key in scoreKeys:\n    print(key,':',scores[key].mean())","metadata":{"execution":{"iopub.status.busy":"2022-07-16T08:29:41.611503Z","iopub.execute_input":"2022-07-16T08:29:41.611924Z","iopub.status.idle":"2022-07-16T08:29:42.114682Z","shell.execute_reply.started":"2022-07-16T08:29:41.611889Z","shell.execute_reply":"2022-07-16T08:29:42.113891Z"},"trusted":true},"execution_count":137,"outputs":[]},{"cell_type":"code","source":"### random forest\nfrom sklearn.ensemble import RandomForestClassifier\n\nRF = RandomForestClassifier(max_depth=8, random_state=0)\nscoring = ['accuracy','precision','precision_macro','recall','recall_macro','roc_auc','f1']\nscores = cross_validate(RF, X, y.ravel(), scoring=scoring)\nscoreKeys = sorted(scores.keys())\nprint('DATA SET WITH BINNING AND NO NORMALIZATION')\nfor key in scoreKeys:\n    print(key,':',scores[key].mean())\n\nprint('---------------------------------------------------------------------------')\nscores = cross_validate(RF, X_norm, y_norm.ravel(), scoring=scoring)\nscoreKeys = sorted(scores.keys())\nprint('DATA SET WITH NO BINNING AND NORMALIZATION')\nfor key in scoreKeys:\n    print(key,':',scores[key].mean())","metadata":{"execution":{"iopub.status.busy":"2022-07-16T08:30:00.536674Z","iopub.execute_input":"2022-07-16T08:30:00.537299Z","iopub.status.idle":"2022-07-16T08:30:07.450845Z","shell.execute_reply.started":"2022-07-16T08:30:00.537266Z","shell.execute_reply":"2022-07-16T08:30:07.449568Z"},"trusted":true},"execution_count":138,"outputs":[]},{"cell_type":"code","source":"### logistic regression\n\nfrom sklearn.linear_model import LogisticRegression\nLR = LogisticRegression(random_state=0,solver='saga',max_iter=500)\nscoring = ['accuracy','precision','precision_macro','recall','recall_macro','roc_auc','f1']\nscores = cross_validate(LR, X, y.ravel(), scoring=scoring)\nscoreKeys = sorted(scores.keys())\nprint('DATA SET WITH BINNING AND NO NORMALIZATION')\nfor key in scoreKeys:\n    print(key,':',scores[key].mean())\n\nprint('---------------------------------------------------------------------------')\nscores = cross_validate(LR, X_norm, y_norm.ravel(), scoring=scoring)\nscoreKeys = sorted(scores.keys())\nprint('DATA SET WITH NO BINNING AND NORMALIZATION')\nfor key in scoreKeys:\n    print(key,':',scores[key].mean())","metadata":{"execution":{"iopub.status.busy":"2022-07-16T08:30:20.068549Z","iopub.execute_input":"2022-07-16T08:30:20.069276Z","iopub.status.idle":"2022-07-16T08:30:27.559071Z","shell.execute_reply.started":"2022-07-16T08:30:20.069239Z","shell.execute_reply":"2022-07-16T08:30:27.557537Z"},"trusted":true},"execution_count":139,"outputs":[]},{"cell_type":"code","source":"### Gaussian NB\nfrom sklearn.naive_bayes import GaussianNB\n\nGNB = GaussianNB()\nscoring = ['accuracy','precision','precision_macro','recall','recall_macro','roc_auc','f1']\nscores = cross_validate(GNB, X, y.ravel(), scoring=scoring)\nscoreKeys = sorted(scores.keys())\nprint('DATA SET WITH BINNING AND NO NORMALIZATION')\nfor key in scoreKeys:\n    print(key,':',scores[key].mean())\n\n    \nprint('---------------------------------------------------------------------------')\nscores = cross_validate(GNB, X_norm, y_norm.ravel(), scoring=scoring)\nscoreKeys = sorted(scores.keys())\nprint('DATA SET WITH NO BINNING AND NORMALIZATION')\nfor key in scoreKeys:\n    print(key,':',scores[key].mean())","metadata":{"execution":{"iopub.status.busy":"2022-07-16T08:30:36.583667Z","iopub.execute_input":"2022-07-16T08:30:36.584102Z","iopub.status.idle":"2022-07-16T08:30:36.799547Z","shell.execute_reply.started":"2022-07-16T08:30:36.584066Z","shell.execute_reply":"2022-07-16T08:30:36.798336Z"},"trusted":true},"execution_count":140,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\n\nXGB = GradientBoostingClassifier(n_estimators=100, learning_rate=1.5,max_depth=3, random_state=0)\nscoring = ['accuracy','precision','precision_macro','recall','recall_macro','roc_auc','f1']\nscores = cross_validate(XGB, X, y, scoring=scoring)\nscoreKeys = sorted(scores.keys())\nprint('DATA SET WITH BINNING AND NO NORMALIZATION')\nfor key in scoreKeys:\n    print(key,':',scores[key].mean())\n\nprint('---------------------------------------------------------------------------')\nscores = cross_validate(XGB, X_norm, y_norm.ravel(), scoring=scoring)\nscoreKeys = sorted(scores.keys())\nprint('DATA SET WITH NO BINNING AND NORMALIZATION')\nfor key in scoreKeys:\n    print(key,':',scores[key].mean())","metadata":{"execution":{"iopub.status.busy":"2022-07-16T08:31:02.936257Z","iopub.execute_input":"2022-07-16T08:31:02.937041Z","iopub.status.idle":"2022-07-16T08:31:12.039019Z","shell.execute_reply.started":"2022-07-16T08:31:02.936987Z","shell.execute_reply":"2022-07-16T08:31:12.037876Z"},"trusted":true},"execution_count":141,"outputs":[]},{"cell_type":"markdown","source":"<h1> From the above models the best performing model was: Random Forest\n    <h2>Once with Binned dataset and not normalized one\n    <h2>Then with Normalized but not binned\n    <h2>It yeilded the following scores:\n    <h3>DATA SET WITH BINNING AND NO NORMALIZATION\n    <h3>test_accuracy : 0.855562553578614\n    <h3>test_f1 : 0.5242486500790025\n    <h3>test_precision : 0.804694724619009\n    <h3>test_precision_macro : 0.8329500236088124\n    <h3>test_recall : 0.388999582340368\n    <h3>test_recall_macro : 0.6823200913743958\n    <h3>test_roc_auc : 0.8550440033383303\n    <h3>---------------------------------------------------------------------------\n    <h3>DATA SET WITH NO BINNING AND NORMALIZATION\n    <h3>test_accuracy : 0.8620540947182421\n    <h3>test_f1 : 0.5570555332233829\n    <h3>test_precision : 0.8131918112086078\n    <h3>test_precision_macro : 0.8406216257238357\n    <h3>test_recall : 0.4241861779230033\n    <h3>test_recall_macro : 0.6994670418887529\n    <h3>test_roc_auc : 0.8596263124908884","metadata":{}}]}